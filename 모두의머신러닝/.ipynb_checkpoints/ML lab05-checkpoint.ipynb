{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "몇 번 반복했나 :  0  학습결과 손실함수 :  2.206808\n",
      "몇 번 반복했나 :  200  학습결과 손실함수 :  0.9576332\n",
      "몇 번 반복했나 :  400  학습결과 손실함수 :  0.6244152\n",
      "몇 번 반복했나 :  600  학습결과 손실함수 :  0.49068663\n",
      "몇 번 반복했나 :  800  학습결과 손실함수 :  0.43287596\n",
      "몇 번 반복했나 :  1000  학습결과 손실함수 :  0.40173778\n",
      "몇 번 반복했나 :  1200  학습결과 손실함수 :  0.3811978\n",
      "몇 번 반복했나 :  1400  학습결과 손실함수 :  0.36550927\n",
      "몇 번 반복했나 :  1600  학습결과 손실함수 :  0.35235476\n",
      "몇 번 반복했나 :  1800  학습결과 손실함수 :  0.34070122\n",
      "몇 번 반복했나 :  2000  학습결과 손실함수 :  0.33004883\n",
      "몇 번 반복했나 :  2200  학습결과 손실함수 :  0.32013753\n",
      "몇 번 반복했나 :  2400  학습결과 손실함수 :  0.31082234\n",
      "몇 번 반복했나 :  2600  학습결과 손실함수 :  0.30201563\n",
      "몇 번 반복했나 :  2800  학습결과 손실함수 :  0.29366025\n",
      "몇 번 반복했나 :  3000  학습결과 손실함수 :  0.2857152\n",
      "몇 번 반복했나 :  3200  학습결과 손실함수 :  0.27814907\n",
      "몇 번 반복했나 :  3400  학습결과 손실함수 :  0.2709359\n",
      "몇 번 반복했나 :  3600  학습결과 손실함수 :  0.2640532\n",
      "몇 번 반복했나 :  3800  학습결과 손실함수 :  0.2574809\n",
      "몇 번 반복했나 :  4000  학습결과 손실함수 :  0.25120077\n",
      "몇 번 반복했나 :  4200  학습결과 손실함수 :  0.24519588\n",
      "몇 번 반복했나 :  4400  학습결과 손실함수 :  0.23945083\n",
      "몇 번 반복했나 :  4600  학습결과 손실함수 :  0.23395081\n",
      "몇 번 반복했나 :  4800  학습결과 손실함수 :  0.22868215\n",
      "몇 번 반복했나 :  5000  학습결과 손실함수 :  0.22363202\n",
      "몇 번 반복했나 :  5200  학습결과 손실함수 :  0.21878843\n",
      "몇 번 반복했나 :  5400  학습결과 손실함수 :  0.21414018\n",
      "몇 번 반복했나 :  5600  학습결과 손실함수 :  0.20967662\n",
      "몇 번 반복했나 :  5800  학습결과 손실함수 :  0.20538789\n",
      "몇 번 반복했나 :  6000  학습결과 손실함수 :  0.20126478\n",
      "몇 번 반복했나 :  6200  학습결과 손실함수 :  0.19729848\n",
      "몇 번 반복했나 :  6400  학습결과 손실함수 :  0.19348092\n",
      "몇 번 반복했나 :  6600  학습결과 손실함수 :  0.18980421\n",
      "몇 번 반복했나 :  6800  학습결과 손실함수 :  0.18626146\n",
      "몇 번 반복했나 :  7000  학습결과 손실함수 :  0.18284571\n",
      "몇 번 반복했나 :  7200  학습결과 손실함수 :  0.17955072\n",
      "몇 번 반복했나 :  7400  학습결과 손실함수 :  0.17637055\n",
      "몇 번 반복했나 :  7600  학습결과 손실함수 :  0.17329954\n",
      "몇 번 반복했나 :  7800  학습결과 손실함수 :  0.17033243\n",
      "몇 번 반복했나 :  8000  학습결과 손실함수 :  0.16746424\n",
      "몇 번 반복했나 :  8200  학습결과 손실함수 :  0.1646904\n",
      "몇 번 반복했나 :  8400  학습결과 손실함수 :  0.16200641\n",
      "몇 번 반복했나 :  8600  학습결과 손실함수 :  0.15940827\n",
      "몇 번 반복했나 :  8800  학습결과 손실함수 :  0.15689187\n",
      "몇 번 반복했나 :  9000  학습결과 손실함수 :  0.15445368\n",
      "몇 번 반복했나 :  9200  학습결과 손실함수 :  0.15209024\n",
      "몇 번 반복했나 :  9400  학습결과 손실함수 :  0.1497982\n",
      "몇 번 반복했나 :  9600  학습결과 손실함수 :  0.1475745\n",
      "몇 번 반복했나 :  9800  학습결과 손실함수 :  0.14541622\n",
      "몇 번 반복했나 :  10000  학습결과 손실함수 :  0.14332049\n",
      "\n",
      "Hypothesis(예측한 값):  [[0.02818643]\n",
      " [0.15523642]\n",
      " [0.2924003 ]\n",
      " [0.7871362 ]\n",
      " [0.9431429 ]\n",
      " [0.9813726 ]] \n",
      "Correct 정답은! :  [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]  \n",
      " Accuracy (진짜 정답이긴 해?):  1.0\n",
      "사실, 원래 x 내용은 .. :  [[1, 2], [2, 3], [3, 1], [4, 3], [5, 3], [6, 2]]\n",
      "사실, 답지 내용은... :  [[0], [0], [0], [1], [1], [1]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "x_data = [[1,2],[2,3],[3,1],[4,3],[5,3],[6,2]]\n",
    "y_data = [[0],[0],[0],[1],[1],[1]]\n",
    "\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 2])\n",
    "Y = tf.placeholder(tf.float32, shape=[None ,1])\n",
    "\n",
    "# 들어오는 값의 개수, 나가는 값의 개수 \n",
    "W = tf.Variable(tf.random_normal([2,1]), name='weight')\n",
    "# bias는 항상 나가는 값의 개수와 같다.\n",
    "b = tf.Variable(tf.random_normal([1]),name='bias')\n",
    "\n",
    "# 예측 값\n",
    "hypothesis = tf.sigmoid(tf.matmul(X,W) + b)\n",
    "\n",
    "# cost 함수를 정의 (reduce_mean은 평균을 구하는 것)\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n",
    "\n",
    "# cost 최소화\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
    "\n",
    "# 이 결과값을 0 아니면 1로 출력을 해 주어야 함.\n",
    "# 결과 값은 float32로 변환을 해 주면 0이나 1로 캐스팅되어 출력이 됨.\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "\n",
    "# 예측한 갑소가 Y(실제 답)의 값이 동일하면 true를 , 다르면 false를 반환한다. -> 이것을 전체 평균.\n",
    "# 평균내어 정확도를 평가할 수 있다.\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
    "\n",
    "# 수행\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(10001):\n",
    "        cost_val, _ = sess.run([cost, train], feed_dict={X: x_data, Y:y_data}) #cost_val\n",
    "        if step % 200 == 0:\n",
    "            print('몇 번 반복했나 : ', step ,' 학습결과 손실함수 : ', cost_val)\n",
    "    \n",
    "    #sess.run은 run 한 노드에 대한 그 노드에서의 결과값? 을 출력하는 듯 하다 -> 찾아보기\n",
    "    # hypothesis : 예측한 값\n",
    "    # predicated : 0인지 1인지 \n",
    "    # accuracy : 정답인지, 오답인지\n",
    "    \n",
    "    h,c,a = sess.run([hypothesis, predicted, accuracy],\n",
    "                    feed_dict={X:x_data, Y:y_data})\n",
    "    print(\"\\nHypothesis(예측한 값): \" ,h, \"\\nCorrect 정답은! : \", c, \" \\n Accuracy (진짜 정답이긴 해?): \",a)\n",
    "    print(\"사실, 원래 x 내용은 .. : \",x_data)\n",
    "    print(\"사실, 답지 내용은... : \", y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 음, 생각보다는 맞추기 어렵지? ㅋㅋㅋㅋ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이걸 가지고 이제 실제 데이터를 가지고 놀아보자.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 당뇨병 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "xy = np.loadtxt('data-03-diabetes.csv', delimiter=',', dtype=np.float32)\n",
    "x_data = xy[:, 0:-1]\n",
    "y_data = xy[:, [-1]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape의 모양에 주의해서 코드를 짜세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32,shape=[None,8]) # 8개의 특징\n",
    "Y = tf.placeholder(tf.float32, shape=[None,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.random_normal([8,1]), name='weight') # 매트릭스 곱을 처리해야 하기 때문에. x가 8개 들어옴.\n",
    "b = tf.Variable(tf.random_normal([1]),name='bias')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothesis = tf.sigmoid(tf.matmul(W,X) + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) *\n",
    "                       tf.log(1 - hypothesis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# true or false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.71554714\n",
      "200 0.64260864\n",
      "400 0.61566144\n",
      "600 0.60178286\n",
      "800 0.59143037\n",
      "1000 0.58245116\n",
      "1200 0.57435787\n",
      "1400 0.5669972\n",
      "1600 0.5602843\n",
      "1800 0.55415255\n",
      "2000 0.5485443\n",
      "2200 0.54340786\n",
      "2400 0.53869736\n",
      "2600 0.5343716\n",
      "2800 0.53039366\n",
      "3000 0.52673084\n",
      "3200 0.5233535\n",
      "3400 0.5202353\n",
      "3600 0.51735264\n",
      "3800 0.51468444\n",
      "4000 0.51221156\n",
      "4200 0.509917\n",
      "4400 0.5077853\n",
      "4600 0.50580275\n",
      "4800 0.50395685\n",
      "5000 0.5022362\n",
      "5200 0.50063056\n",
      "5400 0.49913096\n",
      "5600 0.4977287\n",
      "5800 0.4964165\n",
      "6000 0.4951871\n",
      "6200 0.49403456\n",
      "6400 0.49295282\n",
      "6600 0.49193683\n",
      "6800 0.4909818\n",
      "7000 0.4900832\n",
      "7200 0.48923722\n",
      "7400 0.48844004\n",
      "7600 0.4876883\n",
      "7800 0.48697892\n",
      "8000 0.48630908\n",
      "8200 0.48567608\n",
      "8400 0.4850775\n",
      "8600 0.4845112\n",
      "8800 0.48397493\n",
      "9000 0.48346686\n",
      "9200 0.48298526\n",
      "9400 0.48252842\n",
      "9600 0.48209485\n",
      "9800 0.4816832\n",
      "10000 0.48129204\n",
      "hypothesis (예측) :  Tensor(\"Sigmoid_17:0\", shape=(?, 1), dtype=float32)  predicted(추정!) :  Tensor(\"Cast_30:0\", shape=(?, 1), dtype=float32)  정확해? :  Tensor(\"Mean_34:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    feed = {X:x_data, Y:y_data}\n",
    "    \n",
    "    for step in range(10001):\n",
    "        sess.run(train, feed_dict={X:x_data, Y:y_data})\n",
    "        if step % 200 == 0:\n",
    "            print(step, sess.run(cost, feed_dict={X:x_data, Y:y_data}))\n",
    "            \n",
    "    h, c, a = sess.run([hypothesis, predicted, accuracy], feed_dict={X:x_data, Y:y_data})\n",
    "    print(\"hypothesis (예측) : \", hypothesis, \" predicted(추정!) : \", predicted, \" 정확해? : \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(759, 8) (759, 1)\n",
      "0 0.7161295\n",
      "200 0.64281625\n",
      "400 0.6157496\n",
      "600 0.6018401\n",
      "800 0.59147805\n",
      "1000 0.58249366\n",
      "1200 0.57439643\n",
      "1400 0.56703234\n",
      "1600 0.5603163\n",
      "1800 0.5541819\n",
      "2000 0.5485711\n",
      "2200 0.5434325\n",
      "2400 0.5387199\n",
      "2600 0.53439236\n",
      "2800 0.5304128\n",
      "3000 0.5267483\n",
      "3200 0.5233696\n",
      "3400 0.52025026\n",
      "3600 0.51736647\n",
      "3800 0.5146973\n",
      "4000 0.5122235\n",
      "4200 0.50992805\n",
      "4400 0.50779563\n",
      "4600 0.50581235\n",
      "4800 0.50396574\n",
      "5000 0.5022444\n",
      "5200 0.50063837\n",
      "5400 0.49913818\n",
      "5600 0.49773556\n",
      "5800 0.49642286\n",
      "6000 0.49519315\n",
      "6200 0.4940401\n",
      "6400 0.49295807\n",
      "6600 0.49194178\n",
      "6800 0.4909864\n",
      "7000 0.49008757\n",
      "7200 0.48924133\n",
      "7400 0.48844388\n",
      "7600 0.48769197\n",
      "7800 0.48698243\n",
      "8000 0.48631236\n",
      "8200 0.48567918\n",
      "8400 0.48508048\n",
      "8600 0.48451397\n",
      "8800 0.48397756\n",
      "9000 0.48346937\n",
      "9200 0.48298767\n",
      "9400 0.4825307\n",
      "9600 0.48209703\n",
      "9800 0.4816852\n",
      "10000 0.48129392\n",
      "\n",
      "Hypothesis:  [[0.3981275 ]\n",
      " [0.92527205]\n",
      " [0.24727878]\n",
      " [0.93751574]\n",
      " [0.12775925]\n",
      " [0.7649762 ]\n",
      " [0.935946  ]\n",
      " [0.597498  ]\n",
      " [0.25178915]\n",
      " [0.5091971 ]\n",
      " [0.68982446]\n",
      " [0.17706525]\n",
      " [0.22018188]\n",
      " [0.2972522 ]\n",
      " [0.7212928 ]\n",
      " [0.42357555]\n",
      " [0.72946775]\n",
      " [0.8153833 ]\n",
      " [0.80327886]\n",
      " [0.5654733 ]\n",
      " [0.6281204 ]\n",
      " [0.10807905]\n",
      " [0.69663167]\n",
      " [0.7045945 ]\n",
      " [0.34331858]\n",
      " [0.94439936]\n",
      " [0.62355125]\n",
      " [0.624975  ]\n",
      " [0.67054445]\n",
      " [0.44629225]\n",
      " [0.9599598 ]\n",
      " [0.8824729 ]\n",
      " [0.6119899 ]\n",
      " [0.8146326 ]\n",
      " [0.3764816 ]\n",
      " [0.62861323]\n",
      " [0.7966242 ]\n",
      " [0.4327891 ]\n",
      " [0.50100595]\n",
      " [0.33620423]\n",
      " [0.8604431 ]\n",
      " [0.16084903]\n",
      " [0.41421774]\n",
      " [0.05401632]\n",
      " [0.55555683]\n",
      " [0.9326018 ]\n",
      " [0.70704436]\n",
      " [0.7058152 ]\n",
      " [0.95133907]\n",
      " [0.93763417]\n",
      " [0.9381529 ]\n",
      " [0.24356934]\n",
      " [0.33719438]\n",
      " [0.96474886]\n",
      " [0.21935722]\n",
      " [0.39579642]\n",
      " [0.10018909]\n",
      " [0.70615643]\n",
      " [0.8301487 ]\n",
      " [0.4971636 ]\n",
      " [0.9361135 ]\n",
      " [0.7201288 ]\n",
      " [0.6359305 ]\n",
      " [0.8655842 ]\n",
      " [0.60485154]\n",
      " [0.4815426 ]\n",
      " [0.9643836 ]\n",
      " [0.73846817]\n",
      " [0.84956217]\n",
      " [0.68971634]\n",
      " [0.25034088]\n",
      " [0.7610879 ]\n",
      " [0.9174458 ]\n",
      " [0.93713236]\n",
      " [0.85664433]\n",
      " [0.78696465]\n",
      " [0.4031762 ]\n",
      " [0.8863348 ]\n",
      " [0.92024505]\n",
      " [0.9075958 ]\n",
      " [0.8643824 ]\n",
      " [0.8578192 ]\n",
      " [0.3410614 ]\n",
      " [0.80976886]\n",
      " [0.50947607]\n",
      " [0.86697125]\n",
      " [0.44860247]\n",
      " [0.91472423]\n",
      " [0.9275119 ]\n",
      " [0.7958313 ]\n",
      " [0.76489127]\n",
      " [0.6352471 ]\n",
      " [0.7294727 ]\n",
      " [0.6015572 ]\n",
      " [0.90783215]\n",
      " [0.97832084]\n",
      " [0.89593905]\n",
      " [0.5322862 ]\n",
      " [0.20827451]\n",
      " [0.66399765]\n",
      " [0.6348598 ]\n",
      " [0.96295094]\n",
      " [0.7102358 ]\n",
      " [0.73028696]\n",
      " [0.8956127 ]\n",
      " [0.7030874 ]\n",
      " [0.91542476]\n",
      " [0.8358113 ]\n",
      " [0.53471196]\n",
      " [0.36847073]\n",
      " [0.9308212 ]\n",
      " [0.8530219 ]\n",
      " [0.44106564]\n",
      " [0.40174842]\n",
      " [0.61334825]\n",
      " [0.80515987]\n",
      " [0.88082653]\n",
      " [0.929554  ]\n",
      " [0.11772996]\n",
      " [0.7326119 ]\n",
      " [0.85420525]\n",
      " [0.590847  ]\n",
      " [0.63988805]\n",
      " [0.7254528 ]\n",
      " [0.6796531 ]\n",
      " [0.8343718 ]\n",
      " [0.8113315 ]\n",
      " [0.5698335 ]\n",
      " [0.5752549 ]\n",
      " [0.3727427 ]\n",
      " [0.442427  ]\n",
      " [0.7629223 ]\n",
      " [0.9433515 ]\n",
      " [0.8434399 ]\n",
      " [0.7794283 ]\n",
      " [0.86547637]\n",
      " [0.45625308]\n",
      " [0.80640304]\n",
      " [0.72569287]\n",
      " [0.722383  ]\n",
      " [0.8825445 ]\n",
      " [0.61700183]\n",
      " [0.6051618 ]\n",
      " [0.6995113 ]\n",
      " [0.90465724]\n",
      " [0.70086575]\n",
      " [0.41095042]\n",
      " [0.94360185]\n",
      " [0.6066901 ]\n",
      " [0.7754361 ]\n",
      " [0.26858377]\n",
      " [0.37708837]\n",
      " [0.10783347]\n",
      " [0.2259745 ]\n",
      " [0.90804374]\n",
      " [0.88069147]\n",
      " [0.94828033]\n",
      " [0.11161831]\n",
      " [0.5535065 ]\n",
      " [0.7501422 ]\n",
      " [0.61531156]\n",
      " [0.86011475]\n",
      " [0.42083243]\n",
      " [0.8094474 ]\n",
      " [0.6612494 ]\n",
      " [0.6148763 ]\n",
      " [0.7052539 ]\n",
      " [0.8865869 ]\n",
      " [0.76050377]\n",
      " [0.6436154 ]\n",
      " [0.88692784]\n",
      " [0.8393044 ]\n",
      " [0.947775  ]\n",
      " [0.23329237]\n",
      " [0.793118  ]\n",
      " [0.20697173]\n",
      " [0.34474057]\n",
      " [0.3503008 ]\n",
      " [0.8861053 ]\n",
      " [0.6948271 ]\n",
      " [0.9200715 ]\n",
      " [0.90151376]\n",
      " [0.5954024 ]\n",
      " [0.15599784]\n",
      " [0.19730401]\n",
      " [0.56523794]\n",
      " [0.73984635]\n",
      " [0.6607899 ]\n",
      " [0.8183093 ]\n",
      " [0.60523486]\n",
      " [0.3669251 ]\n",
      " [0.17357683]\n",
      " [0.9123155 ]\n",
      " [0.39052355]\n",
      " [0.8456277 ]\n",
      " [0.91389745]\n",
      " [0.68046397]\n",
      " [0.68042326]\n",
      " [0.63293314]\n",
      " [0.5451133 ]\n",
      " [0.7397036 ]\n",
      " [0.9559865 ]\n",
      " [0.7350151 ]\n",
      " [0.8391576 ]\n",
      " [0.13827378]\n",
      " [0.30907118]\n",
      " [0.8899628 ]\n",
      " [0.23661742]\n",
      " [0.9389515 ]\n",
      " [0.2733169 ]\n",
      " [0.2797521 ]\n",
      " [0.4905479 ]\n",
      " [0.7204721 ]\n",
      " [0.20406571]\n",
      " [0.73552436]\n",
      " [0.7341911 ]\n",
      " [0.80169517]\n",
      " [0.62849116]\n",
      " [0.1488601 ]\n",
      " [0.2996352 ]\n",
      " [0.7122265 ]\n",
      " [0.49803787]\n",
      " [0.9311878 ]\n",
      " [0.93746006]\n",
      " [0.69213265]\n",
      " [0.36798754]\n",
      " [0.03287467]\n",
      " [0.66465044]\n",
      " [0.33520728]\n",
      " [0.42448834]\n",
      " [0.95140815]\n",
      " [0.6160808 ]\n",
      " [0.9491986 ]\n",
      " [0.21691948]\n",
      " [0.16390094]\n",
      " [0.33199427]\n",
      " [0.77713263]\n",
      " [0.90850776]\n",
      " [0.879884  ]\n",
      " [0.6591547 ]\n",
      " [0.6604953 ]\n",
      " [0.5907532 ]\n",
      " [0.17395893]\n",
      " [0.54844594]\n",
      " [0.1455436 ]\n",
      " [0.5950093 ]\n",
      " [0.8904655 ]\n",
      " [0.643093  ]\n",
      " [0.7122218 ]\n",
      " [0.9585595 ]\n",
      " [0.8193933 ]\n",
      " [0.78883094]\n",
      " [0.7260613 ]\n",
      " [0.7587508 ]\n",
      " [0.88067436]\n",
      " [0.46308556]\n",
      " [0.47984752]\n",
      " [0.50213325]\n",
      " [0.81751317]\n",
      " [0.6668086 ]\n",
      " [0.66916305]\n",
      " [0.7946639 ]\n",
      " [0.30652654]\n",
      " [0.44554925]\n",
      " [0.5435424 ]\n",
      " [0.63395935]\n",
      " [0.36887878]\n",
      " [0.89835757]\n",
      " [0.76450324]\n",
      " [0.8994658 ]\n",
      " [0.52998227]\n",
      " [0.7214377 ]\n",
      " [0.8404345 ]\n",
      " [0.84535617]\n",
      " [0.6311217 ]\n",
      " [0.880105  ]\n",
      " [0.35459137]\n",
      " [0.6040208 ]\n",
      " [0.72768533]\n",
      " [0.37278485]\n",
      " [0.78686994]\n",
      " [0.28746632]\n",
      " [0.5573001 ]\n",
      " [0.9468471 ]\n",
      " [0.76558316]\n",
      " [0.8321102 ]\n",
      " [0.6870678 ]\n",
      " [0.42960536]\n",
      " [0.60463667]\n",
      " [0.39861885]\n",
      " [0.47066262]\n",
      " [0.66584986]\n",
      " [0.6672472 ]\n",
      " [0.65392554]\n",
      " [0.57867014]\n",
      " [0.21015951]\n",
      " [0.6921353 ]\n",
      " [0.89411366]\n",
      " [0.45293033]\n",
      " [0.6514549 ]\n",
      " [0.7503762 ]\n",
      " [0.5260521 ]\n",
      " [0.76370496]\n",
      " [0.52409697]\n",
      " [0.6976042 ]\n",
      " [0.8990223 ]\n",
      " [0.6527533 ]\n",
      " [0.7448806 ]\n",
      " [0.87491214]\n",
      " [0.5188885 ]\n",
      " [0.8621626 ]\n",
      " [0.9547938 ]\n",
      " [0.3243977 ]\n",
      " [0.7758979 ]\n",
      " [0.26719332]\n",
      " [0.8052272 ]\n",
      " [0.8273407 ]\n",
      " [0.74462914]\n",
      " [0.39217004]\n",
      " [0.77135825]\n",
      " [0.790106  ]\n",
      " [0.72921735]\n",
      " [0.19916984]\n",
      " [0.802485  ]\n",
      " [0.8531532 ]\n",
      " [0.575787  ]\n",
      " [0.945609  ]\n",
      " [0.23158991]\n",
      " [0.71063626]\n",
      " [0.95562327]\n",
      " [0.22176981]\n",
      " [0.42212397]\n",
      " [0.66124296]\n",
      " [0.33959633]\n",
      " [0.17736146]\n",
      " [0.8719316 ]\n",
      " [0.9156767 ]\n",
      " [0.8576468 ]\n",
      " [0.60939825]\n",
      " [0.60377455]\n",
      " [0.55564123]\n",
      " [0.78739035]\n",
      " [0.82319814]\n",
      " [0.94388604]\n",
      " [0.72454137]\n",
      " [0.73379743]\n",
      " [0.5933361 ]\n",
      " [0.9273713 ]\n",
      " [0.9453502 ]\n",
      " [0.65431565]\n",
      " [0.2828851 ]\n",
      " [0.6525693 ]\n",
      " [0.3558371 ]\n",
      " [0.7327425 ]\n",
      " [0.21125573]\n",
      " [0.28513196]\n",
      " [0.39868996]\n",
      " [0.652068  ]\n",
      " [0.3129916 ]\n",
      " [0.59886837]\n",
      " [0.8373558 ]\n",
      " [0.6572511 ]\n",
      " [0.8723697 ]\n",
      " [0.95470023]\n",
      " [0.7595162 ]\n",
      " [0.07584012]\n",
      " [0.41696963]\n",
      " [0.8237756 ]\n",
      " [0.84930146]\n",
      " [0.63638586]\n",
      " [0.26041642]\n",
      " [0.9083249 ]\n",
      " [0.8808288 ]\n",
      " [0.27464145]\n",
      " [0.5652129 ]\n",
      " [0.83494365]\n",
      " [0.8848278 ]\n",
      " [0.8636601 ]\n",
      " [0.8999337 ]\n",
      " [0.8921491 ]\n",
      " [0.93531406]\n",
      " [0.6851334 ]\n",
      " [0.59651965]\n",
      " [0.55309963]\n",
      " [0.8304032 ]\n",
      " [0.8650349 ]\n",
      " [0.22632152]\n",
      " [0.8262229 ]\n",
      " [0.8943026 ]\n",
      " [0.3204102 ]\n",
      " [0.6084472 ]\n",
      " [0.8483672 ]\n",
      " [0.5833251 ]\n",
      " [0.9111651 ]\n",
      " [0.3209038 ]\n",
      " [0.8206371 ]\n",
      " [0.62017393]\n",
      " [0.88247716]\n",
      " [0.36368155]\n",
      " [0.6753888 ]\n",
      " [0.69491255]\n",
      " [0.8032275 ]\n",
      " [0.10435256]\n",
      " [0.20804727]\n",
      " [0.6736816 ]\n",
      " [0.8161435 ]\n",
      " [0.4571012 ]\n",
      " [0.7962928 ]\n",
      " [0.47205505]\n",
      " [0.4115871 ]\n",
      " [0.84986514]\n",
      " [0.45825544]\n",
      " [0.915504  ]\n",
      " [0.8351885 ]\n",
      " [0.6327107 ]\n",
      " [0.9133854 ]\n",
      " [0.6002363 ]\n",
      " [0.793143  ]\n",
      " [0.3373121 ]\n",
      " [0.30507827]\n",
      " [0.74524516]\n",
      " [0.44833043]\n",
      " [0.46922037]\n",
      " [0.889863  ]\n",
      " [0.89991534]\n",
      " [0.9113413 ]\n",
      " [0.9537049 ]\n",
      " [0.7041675 ]\n",
      " [0.89460564]\n",
      " [0.35932723]\n",
      " [0.39278334]\n",
      " [0.4917079 ]\n",
      " [0.94797325]\n",
      " [0.565993  ]\n",
      " [0.2136006 ]\n",
      " [0.92882335]\n",
      " [0.8276244 ]\n",
      " [0.54783213]\n",
      " [0.832452  ]\n",
      " [0.01257187]\n",
      " [0.92746603]\n",
      " [0.7604475 ]\n",
      " [0.75701755]\n",
      " [0.8025802 ]\n",
      " [0.9693765 ]\n",
      " [0.63401973]\n",
      " [0.77184474]\n",
      " [0.67294407]\n",
      " [0.8456502 ]\n",
      " [0.23651832]\n",
      " [0.5642228 ]\n",
      " [0.9117722 ]\n",
      " [0.57456595]\n",
      " [0.76535064]\n",
      " [0.9379421 ]\n",
      " [0.81576574]\n",
      " [0.8870805 ]\n",
      " [0.51860607]\n",
      " [0.80985284]\n",
      " [0.9516673 ]\n",
      " [0.738505  ]\n",
      " [0.6391387 ]\n",
      " [0.30174744]\n",
      " [0.45324767]\n",
      " [0.57301337]\n",
      " [0.6486947 ]\n",
      " [0.5199462 ]\n",
      " [0.78229856]\n",
      " [0.59994817]\n",
      " [0.75291365]\n",
      " [0.8567492 ]\n",
      " [0.7542268 ]\n",
      " [0.6410285 ]\n",
      " [0.50999975]\n",
      " [0.61479723]\n",
      " [0.9385299 ]\n",
      " [0.84671074]\n",
      " [0.23596376]\n",
      " [0.43970308]\n",
      " [0.4478876 ]\n",
      " [0.0921506 ]\n",
      " [0.9057876 ]\n",
      " [0.14710107]\n",
      " [0.8900614 ]\n",
      " [0.8690343 ]\n",
      " [0.8306978 ]\n",
      " [0.6749864 ]\n",
      " [0.87900317]\n",
      " [0.34150562]\n",
      " [0.7755265 ]\n",
      " [0.94163316]\n",
      " [0.34519938]\n",
      " [0.44899353]\n",
      " [0.8791895 ]\n",
      " [0.86791104]\n",
      " [0.60994446]\n",
      " [0.7984794 ]\n",
      " [0.79338145]\n",
      " [0.7875385 ]\n",
      " [0.32020247]\n",
      " [0.7488952 ]\n",
      " [0.875285  ]\n",
      " [0.5914097 ]\n",
      " [0.79626155]\n",
      " [0.75001544]\n",
      " [0.7913364 ]\n",
      " [0.8552245 ]\n",
      " [0.9405463 ]\n",
      " [0.6569216 ]\n",
      " [0.41087294]\n",
      " [0.7576697 ]\n",
      " [0.7915181 ]\n",
      " [0.96717453]\n",
      " [0.77577394]\n",
      " [0.6922852 ]\n",
      " [0.38295645]\n",
      " [0.71582234]\n",
      " [0.9242936 ]\n",
      " [0.9512239 ]\n",
      " [0.9142939 ]\n",
      " [0.7250816 ]\n",
      " [0.65968084]\n",
      " [0.8234351 ]\n",
      " [0.43695956]\n",
      " [0.75030214]\n",
      " [0.78048277]\n",
      " [0.87229717]\n",
      " [0.6141111 ]\n",
      " [0.6947192 ]\n",
      " [0.86114573]\n",
      " [0.49841112]\n",
      " [0.4887157 ]\n",
      " [0.6163386 ]\n",
      " [0.74332595]\n",
      " [0.62416977]\n",
      " [0.9054115 ]\n",
      " [0.91966105]\n",
      " [0.23768216]\n",
      " [0.12826559]\n",
      " [0.78810203]\n",
      " [0.5000277 ]\n",
      " [0.27804565]\n",
      " [0.8381753 ]\n",
      " [0.89868844]\n",
      " [0.6600512 ]\n",
      " [0.9361907 ]\n",
      " [0.9094714 ]\n",
      " [0.78259015]\n",
      " [0.8231935 ]\n",
      " [0.6678024 ]\n",
      " [0.535326  ]\n",
      " [0.7565763 ]\n",
      " [0.5769422 ]\n",
      " [0.14117873]\n",
      " [0.8915922 ]\n",
      " [0.8899046 ]\n",
      " [0.7020565 ]\n",
      " [0.92382765]\n",
      " [0.8344708 ]\n",
      " [0.8756228 ]\n",
      " [0.60877895]\n",
      " [0.7094668 ]\n",
      " [0.8629633 ]\n",
      " [0.7193383 ]\n",
      " [0.86056507]\n",
      " [0.9161494 ]\n",
      " [0.61133885]\n",
      " [0.78151   ]\n",
      " [0.8359717 ]\n",
      " [0.46535575]\n",
      " [0.5443123 ]\n",
      " [0.06288207]\n",
      " [0.24365097]\n",
      " [0.8559542 ]\n",
      " [0.6732842 ]\n",
      " [0.6435909 ]\n",
      " [0.57070124]\n",
      " [0.9475695 ]\n",
      " [0.4368164 ]\n",
      " [0.8138957 ]\n",
      " [0.26507083]\n",
      " [0.8972534 ]\n",
      " [0.2965218 ]\n",
      " [0.7543652 ]\n",
      " [0.54288447]\n",
      " [0.8388685 ]\n",
      " [0.56029195]\n",
      " [0.31277752]\n",
      " [0.7373237 ]\n",
      " [0.9275844 ]\n",
      " [0.376783  ]\n",
      " [0.92789364]\n",
      " [0.8890158 ]\n",
      " [0.85257345]\n",
      " [0.82325864]\n",
      " [0.40795636]\n",
      " [0.34046626]\n",
      " [0.6610166 ]\n",
      " [0.17593119]\n",
      " [0.95339715]\n",
      " [0.3638856 ]\n",
      " [0.9396896 ]\n",
      " [0.8900349 ]\n",
      " [0.45002332]\n",
      " [0.20239806]\n",
      " [0.70377266]\n",
      " [0.44354326]\n",
      " [0.834051  ]\n",
      " [0.717277  ]\n",
      " [0.9820377 ]\n",
      " [0.53854024]\n",
      " [0.6451048 ]\n",
      " [0.76880634]\n",
      " [0.79318786]\n",
      " [0.06311718]\n",
      " [0.71731913]\n",
      " [0.7743789 ]\n",
      " [0.82007074]\n",
      " [0.62836957]\n",
      " [0.44392955]\n",
      " [0.5811739 ]\n",
      " [0.9070859 ]\n",
      " [0.6163922 ]\n",
      " [0.7817997 ]\n",
      " [0.8134316 ]\n",
      " [0.886778  ]\n",
      " [0.8046607 ]\n",
      " [0.56241566]\n",
      " [0.77090585]\n",
      " [0.9037728 ]\n",
      " [0.6718225 ]\n",
      " [0.96506906]\n",
      " [0.8205715 ]\n",
      " [0.61710966]\n",
      " [0.48430187]\n",
      " [0.8098373 ]\n",
      " [0.84849584]\n",
      " [0.49826664]\n",
      " [0.7107854 ]\n",
      " [0.26749313]\n",
      " [0.5949287 ]\n",
      " [0.8455515 ]\n",
      " [0.95075   ]\n",
      " [0.83687437]\n",
      " [0.7288989 ]\n",
      " [0.74641144]\n",
      " [0.8914495 ]\n",
      " [0.53175724]\n",
      " [0.93602085]\n",
      " [0.49140215]\n",
      " [0.8193039 ]\n",
      " [0.32551783]\n",
      " [0.08307382]\n",
      " [0.29210496]\n",
      " [0.33152634]\n",
      " [0.691342  ]\n",
      " [0.81725144]\n",
      " [0.5929615 ]\n",
      " [0.77059025]\n",
      " [0.79304487]\n",
      " [0.53440624]\n",
      " [0.38611388]\n",
      " [0.903097  ]\n",
      " [0.8875451 ]\n",
      " [0.3578655 ]\n",
      " [0.5948596 ]\n",
      " [0.20892039]\n",
      " [0.4054815 ]\n",
      " [0.73219794]\n",
      " [0.6953239 ]\n",
      " [0.9123838 ]\n",
      " [0.97678685]\n",
      " [0.2043218 ]\n",
      " [0.7053372 ]\n",
      " [0.6003292 ]\n",
      " [0.37993342]\n",
      " [0.73580235]\n",
      " [0.7477368 ]\n",
      " [0.9041373 ]\n",
      " [0.75090706]\n",
      " [0.4348921 ]\n",
      " [0.6421413 ]\n",
      " [0.16526186]\n",
      " [0.6597425 ]\n",
      " [0.4894041 ]\n",
      " [0.9065212 ]\n",
      " [0.61792946]\n",
      " [0.62564224]\n",
      " [0.8029311 ]\n",
      " [0.7555363 ]\n",
      " [0.4157354 ]\n",
      " [0.7577685 ]\n",
      " [0.65012586]\n",
      " [0.3170253 ]\n",
      " [0.56491613]\n",
      " [0.9006238 ]\n",
      " [0.83290815]\n",
      " [0.5889353 ]\n",
      " [0.8086529 ]\n",
      " [0.30172557]\n",
      " [0.8303797 ]\n",
      " [0.6649468 ]\n",
      " [0.76293737]\n",
      " [0.41015986]\n",
      " [0.6816125 ]\n",
      " [0.8267422 ]\n",
      " [0.18722296]\n",
      " [0.2972168 ]\n",
      " [0.8164085 ]\n",
      " [0.79470205]\n",
      " [0.77619755]\n",
      " [0.9138734 ]\n",
      " [0.7521957 ]\n",
      " [0.7038621 ]\n",
      " [0.71922964]\n",
      " [0.7421838 ]\n",
      " [0.6779882 ]\n",
      " [0.7923618 ]\n",
      " [0.55181795]\n",
      " [0.493759  ]\n",
      " [0.8774132 ]\n",
      " [0.81417763]\n",
      " [0.66732013]\n",
      " [0.28689432]\n",
      " [0.88503444]\n",
      " [0.78499055]\n",
      " [0.832877  ]\n",
      " [0.67723256]\n",
      " [0.87854505]\n",
      " [0.8519352 ]\n",
      " [0.7264064 ]\n",
      " [0.38074407]\n",
      " [0.89722633]\n",
      " [0.9218601 ]\n",
      " [0.3182008 ]\n",
      " [0.15571514]\n",
      " [0.7242738 ]\n",
      " [0.38513738]\n",
      " [0.7306491 ]\n",
      " [0.36243516]\n",
      " [0.45818624]\n",
      " [0.41310877]\n",
      " [0.7581745 ]\n",
      " [0.88684666]\n",
      " [0.16205537]\n",
      " [0.39920557]\n",
      " [0.5614491 ]\n",
      " [0.5113038 ]\n",
      " [0.48278537]\n",
      " [0.7685137 ]\n",
      " [0.15137097]\n",
      " [0.91424584]\n",
      " [0.196005  ]\n",
      " [0.8520802 ]\n",
      " [0.6796559 ]\n",
      " [0.73930275]\n",
      " [0.85132694]\n",
      " [0.6794384 ]\n",
      " [0.89822626]] \n",
      "Correct (Y):  [[0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] \n",
      "Accuracy:  0.770751\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "tf.set_random_seed(777)  # for reproducibility\n",
    "\n",
    "xy = np.loadtxt('data-03-diabetes.csv', delimiter=',', dtype=np.float32)\n",
    "x_data = xy[:, 0:-1]\n",
    "y_data = xy[:, [-1]]\n",
    "\n",
    "print(x_data.shape, y_data.shape)\n",
    "\n",
    "# placeholders for a tensor that will be always fed.\n",
    "X = tf.placeholder(tf.float32, shape=[None, 8])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([8, 1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "\n",
    "# Hypothesis using sigmoid: tf.div(1., 1. + tf.exp(-tf.matmul(X, W)))\n",
    "hypothesis = tf.sigmoid(tf.matmul(X, W) + b)\n",
    "\n",
    "# cost/loss function\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) *\n",
    "                       tf.log(1 - hypothesis))\n",
    "\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
    "\n",
    "# Accuracy computation\n",
    "# True if hypothesis>0.5 else False\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
    "\n",
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    # Initialize TensorFlow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(10001):\n",
    "        cost_val, _ = sess.run([cost, train], feed_dict={X: x_data, Y: y_data})\n",
    "        if step % 200 == 0:\n",
    "            print(step, cost_val)\n",
    "\n",
    "    # Accuracy report\n",
    "    h, c, a = sess.run([hypothesis, predicted, accuracy],\n",
    "                       feed_dict={X: x_data, Y: y_data})\n",
    "    print(\"\\nHypothesis: \", h, \"\\nCorrect (Y): \", c, \"\\nAccuracy: \", a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
