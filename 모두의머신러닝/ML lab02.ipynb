{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 21.20761 [-0.8958462] [-0.54552495]\n",
      "20 0.19981806 [0.7282792] [0.15537393]\n",
      "40 0.008654886 [0.8872133] [0.21236981]\n",
      "60 0.006293585 [0.90643144] [0.20851158]\n",
      "80 0.0057017314 [0.9121541] [0.19929533]\n",
      "100 0.005178265 [0.91640866] [0.1899847]\n",
      "120 0.00470298 [0.92034924] [0.1810614]\n",
      "140 0.004271338 [0.9240936] [0.17255269]\n",
      "160 0.0038792829 [0.92766106] [0.16444342]\n",
      "180 0.0035232324 [0.9310608] [0.15671514]\n",
      "200 0.003199858 [0.93430066] [0.14935012]\n",
      "220 0.0029061611 [0.93738824] [0.14233118]\n",
      "240 0.0026394178 [0.94033074] [0.13564217]\n",
      "260 0.0023971642 [0.943135] [0.1292675]\n",
      "280 0.0021771472 [0.94580746] [0.12319242]\n",
      "300 0.0019773177 [0.94835424] [0.11740284]\n",
      "320 0.0017958283 [0.95078146] [0.11188535]\n",
      "340 0.001631001 [0.95309454] [0.10662713]\n",
      "360 0.0014813015 [0.95529896] [0.10161602]\n",
      "380 0.0013453481 [0.9573997] [0.09684045]\n",
      "400 0.0012218622 [0.95940167] [0.09228937]\n",
      "420 0.0011097172 [0.9613097] [0.08795212]\n",
      "440 0.0010078655 [0.96312803] [0.08381873]\n",
      "460 0.00091535616 [0.9648609] [0.07987953]\n",
      "480 0.00083134085 [0.9665123] [0.07612546]\n",
      "500 0.0007550355 [0.96808606] [0.07254783]\n",
      "520 0.00068573677 [0.96958596] [0.06913834]\n",
      "540 0.0006227957 [0.97101533] [0.06588905]\n",
      "560 0.00056563265 [0.9723775] [0.06279249]\n",
      "580 0.00051372027 [0.9736756] [0.0598415]\n",
      "600 0.0004665636 [0.97491276] [0.05702918]\n",
      "620 0.00042374455 [0.9760918] [0.05434902]\n",
      "640 0.00038484833 [0.9772154] [0.05179479]\n",
      "660 0.00034952676 [0.97828615] [0.04936063]\n",
      "680 0.0003174446 [0.9793067] [0.04704086]\n",
      "700 0.00028830676 [0.98027927] [0.04483005]\n",
      "720 0.00026184446 [0.981206] [0.04272318]\n",
      "740 0.00023781217 [0.9820893] [0.04071534]\n",
      "760 0.00021598434 [0.98293096] [0.03880187]\n",
      "780 0.00019616245 [0.9837332] [0.03697834]\n",
      "800 0.00017815731 [0.9844976] [0.0352405]\n",
      "820 0.00016180397 [0.9852262] [0.03358434]\n",
      "840 0.0001469531 [0.9859205] [0.032006]\n",
      "860 0.00013346714 [0.98658216] [0.03050188]\n",
      "880 0.00012121582 [0.9872127] [0.0290684]\n",
      "900 0.00011009102 [0.9878137] [0.02770231]\n",
      "920 9.998623e-05 [0.98838645] [0.02640042]\n",
      "940 9.08092e-05 [0.9889322] [0.0251597]\n",
      "960 8.2475606e-05 [0.9894523] [0.02397731]\n",
      "980 7.490477e-05 [0.9899479] [0.02285053]\n",
      "1000 6.802964e-05 [0.9904205] [0.02177659]\n",
      "1020 6.1785475e-05 [0.9908707] [0.02075314]\n",
      "1040 5.611384e-05 [0.99129975] [0.01977779]\n",
      "1060 5.0964736e-05 [0.99170864] [0.01884828]\n",
      "1080 4.628695e-05 [0.9920983] [0.01796247]\n",
      "1100 4.203789e-05 [0.9924696] [0.01711829]\n",
      "1120 3.8180126e-05 [0.99282354] [0.01631381]\n",
      "1140 3.467523e-05 [0.9931608] [0.01554711]\n",
      "1160 3.1491745e-05 [0.99348223] [0.01481644]\n",
      "1180 2.8602188e-05 [0.99378854] [0.01412012]\n",
      "1200 2.5976746e-05 [0.9940804] [0.01345652]\n",
      "1220 2.3592655e-05 [0.9943586] [0.01282414]\n",
      "1240 2.142705e-05 [0.9946237] [0.01222147]\n",
      "1260 1.9460347e-05 [0.99487644] [0.01164711]\n",
      "1280 1.7674634e-05 [0.99511725] [0.0110997]\n",
      "1300 1.605162e-05 [0.99534667] [0.01057805]\n",
      "1320 1.45788945e-05 [0.99556535] [0.01008094]\n",
      "1340 1.3240892e-05 [0.9957738] [0.00960718]\n",
      "1360 1.2025591e-05 [0.9959724] [0.00915569]\n",
      "1380 1.0922134e-05 [0.99616164] [0.00872541]\n",
      "1400 9.9194485e-06 [0.99634206] [0.00831535]\n",
      "1420 9.008659e-06 [0.99651396] [0.00792457]\n",
      "1440 8.181856e-06 [0.9966778] [0.00755214]\n",
      "1460 7.4311756e-06 [0.996834] [0.0071972]\n",
      "1480 6.7486653e-06 [0.9969827] [0.00685896]\n",
      "1500 6.129532e-06 [0.99712455] [0.00653663]\n",
      "1520 5.5670002e-06 [0.9972597] [0.00622942]\n",
      "1540 5.0560407e-06 [0.9973884] [0.00593669]\n",
      "1560 4.592019e-06 [0.9975112] [0.00565769]\n",
      "1580 4.1704243e-06 [0.99762815] [0.00539179]\n",
      "1600 3.7877896e-06 [0.9977397] [0.00513838]\n",
      "1620 3.4402294e-06 [0.9978458] [0.00489688]\n",
      "1640 3.124149e-06 [0.99794704] [0.00466677]\n",
      "1660 2.8375582e-06 [0.99804354] [0.00444747]\n",
      "1680 2.577214e-06 [0.9981355] [0.00423846]\n",
      "1700 2.340423e-06 [0.9982231] [0.00403926]\n",
      "1720 2.1256728e-06 [0.9983066] [0.00384945]\n",
      "1740 1.930689e-06 [0.99838614] [0.00366858]\n",
      "1760 1.7535198e-06 [0.99846196] [0.0034962]\n",
      "1780 1.5926613e-06 [0.99853426] [0.00333192]\n",
      "1800 1.4464358e-06 [0.99860317] [0.00317532]\n",
      "1820 1.3136678e-06 [0.9986688] [0.00302611]\n",
      "1840 1.1930767e-06 [0.9987313] [0.0028839]\n",
      "1860 1.083648e-06 [0.9987909] [0.0027484]\n",
      "1880 9.840724e-07 [0.9988478] [0.00261926]\n",
      "1900 8.9379586e-07 [0.9989019] [0.0024962]\n",
      "1920 8.1181935e-07 [0.9989535] [0.0023789]\n",
      "1940 7.3734105e-07 [0.9990027] [0.0022671]\n",
      "1960 6.697107e-07 [0.99904954] [0.00216055]\n",
      "1980 6.082749e-07 [0.9990942] [0.00205906]\n",
      "2000 5.523861e-07 [0.9991367] [0.00196233]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# trainable 한 내용이다. -> 텐서플로우가 변경을 시키낟.\n",
    "x_train = [1,2,3]\n",
    "y_train = [1,2,3]\n",
    "\n",
    "#Variable : 텐서플로가 사용하는 변수이다.\n",
    "#텐서 플로우가 자체적으로 변경시키는 변수이다.\n",
    "\n",
    "# W : 값이 하나인 1차원 array. random 값이며 이름은 weight\n",
    "W = tf.Variable(tf.random_normal([1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "\n",
    "#node 생성\n",
    "hypothesis = x_train * W + b\n",
    "\n",
    "\n",
    "#cost 함수 : reduce_mean :  평균을 내주는 역할.\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - y_train))\n",
    "\n",
    "\n",
    "#cost를 최소화 하자.\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "\n",
    "#무엇을 최소화 할 것인가 -cost를 최소화\n",
    "# 텐서는 w와 b를 조정해서 최소화를 한다.\n",
    "# 그래프 상의 한 노드의 이름 : train\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "\n",
    "#실행하려면 세션을 만들어야 한다.\n",
    "sess = tf.Session()\n",
    "\n",
    "# variable을 사용하기 전에는 꼭 global_variables_initializer()로 변수를 초기화 해야한다.\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "#fit the line\n",
    "for step in range(2001):\n",
    "    #학습을하는 노드 train을 실행시키는데, 2000.\n",
    "    #학습의 발생\n",
    "    sess.run(train)\n",
    "    if step %20 == 0:\n",
    "        print(step, sess.run(cost), sess.run(W), sess.run(b))\n",
    "        \n",
    "        \n",
    "##@@ cost가 하나의 노드 ?? 이 노드들 간의 연결은 어떻게 되는 건가."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.5\n",
      "[3. 7.]\n"
     ]
    }
   ],
   "source": [
    "#place holders\n",
    "#값을 해당 노드 사용 시에 직접 지정해 줄 수 있다.\n",
    "# node a : 새로운 값을 입력받는다.\n",
    "# node b : 새로운 값을 입력받는다.\\\n",
    "# node adder_node : + 연산을 처리하는 노드\n",
    "\n",
    "a = tf.placeholder(tf.float32)\n",
    "b = tf.placeholder(tf.float32)\n",
    "adder_node = a + b\n",
    "\n",
    "print(sess.run(adder_node, feed_dict={a:3,b:4.5}))\n",
    "print(sess.run(adder_node, feed_dict={a:[1,3], b:[2,4]}))\n",
    "#이런 식으로 처리할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 cost :  0.07586564  Weight :  [1.1067675]  B :  [0.0153512]\n",
      "20 cost :  0.000778787  Weight :  [1.0201619]  B :  [-0.02126081]\n",
      "40 cost :  8.966104e-05  Weight :  [1.0114456]  B :  [-0.02367904]\n",
      "60 cost :  7.583212e-05  Weight :  [1.010168]  B :  [-0.02289153]\n",
      "80 cost :  6.882026e-05  Weight :  [1.0096198]  B :  [-0.02184667]\n",
      "100 cost :  6.250409e-05  Weight :  [1.009161]  B :  [-0.02082292]\n",
      "120 cost :  5.6766672e-05  Weight :  [1.0087297]  B :  [-0.01984461]\n",
      "140 cost :  5.1556137e-05  Weight :  [1.0083194]  B :  [-0.01891197]\n",
      "160 cost :  4.682386e-05  Weight :  [1.0079284]  B :  [-0.01802313]\n",
      "180 cost :  4.252714e-05  Weight :  [1.0075558]  B :  [-0.01717613]\n",
      "200 cost :  3.8623395e-05  Weight :  [1.0072007]  B :  [-0.01636894]\n",
      "220 cost :  3.5078734e-05  Weight :  [1.0068624]  B :  [-0.01559969]\n",
      "240 cost :  3.1859567e-05  Weight :  [1.0065398]  B :  [-0.01486663]\n",
      "260 cost :  2.893467e-05  Weight :  [1.0062325]  B :  [-0.01416791]\n",
      "280 cost :  2.627876e-05  Weight :  [1.0059395]  B :  [-0.01350203]\n",
      "300 cost :  2.3867067e-05  Weight :  [1.0056604]  B :  [-0.01286746]\n",
      "320 cost :  2.1676562e-05  Weight :  [1.0053945]  B :  [-0.01226276]\n",
      "340 cost :  1.9686957e-05  Weight :  [1.005141]  B :  [-0.01168649]\n",
      "360 cost :  1.7880053e-05  Weight :  [1.0048994]  B :  [-0.01113731]\n",
      "380 cost :  1.6239355e-05  Weight :  [1.0046691]  B :  [-0.01061391]\n",
      "400 cost :  1.4748905e-05  Weight :  [1.0044497]  B :  [-0.01011512]\n",
      "420 cost :  1.3395227e-05  Weight :  [1.0042405]  B :  [-0.00963976]\n",
      "440 cost :  1.2165315e-05  Weight :  [1.0040413]  B :  [-0.00918675]\n",
      "460 cost :  1.1048895e-05  Weight :  [1.0038513]  B :  [-0.00875498]\n",
      "480 cost :  1.0034314e-05  Weight :  [1.0036703]  B :  [-0.00834351]\n",
      "500 cost :  9.114007e-06  Weight :  [1.0034978]  B :  [-0.00795142]\n",
      "520 cost :  8.277296e-06  Weight :  [1.0033334]  B :  [-0.00757774]\n",
      "540 cost :  7.5176795e-06  Weight :  [1.0031768]  B :  [-0.00722162]\n",
      "560 cost :  6.827515e-06  Weight :  [1.0030276]  B :  [-0.00688222]\n",
      "580 cost :  6.201051e-06  Weight :  [1.0028853]  B :  [-0.00655883]\n",
      "600 cost :  5.63204e-06  Weight :  [1.0027497]  B :  [-0.00625064]\n",
      "620 cost :  5.115375e-06  Weight :  [1.0026206]  B :  [-0.00595694]\n",
      "640 cost :  4.6456976e-06  Weight :  [1.0024974]  B :  [-0.00567705]\n",
      "660 cost :  4.2195693e-06  Weight :  [1.0023801]  B :  [-0.00541033]\n",
      "680 cost :  3.8324956e-06  Weight :  [1.0022683]  B :  [-0.00515612]\n",
      "700 cost :  3.4806808e-06  Weight :  [1.0021616]  B :  [-0.00491386]\n",
      "720 cost :  3.161181e-06  Weight :  [1.00206]  B :  [-0.00468297]\n",
      "740 cost :  2.8712486e-06  Weight :  [1.0019633]  B :  [-0.00446293]\n",
      "760 cost :  2.607848e-06  Weight :  [1.0018711]  B :  [-0.00425325]\n",
      "780 cost :  2.3683804e-06  Weight :  [1.0017833]  B :  [-0.00405342]\n",
      "800 cost :  2.1509911e-06  Weight :  [1.0016994]  B :  [-0.00386297]\n",
      "820 cost :  1.9535548e-06  Weight :  [1.0016196]  B :  [-0.00368152]\n",
      "840 cost :  1.7745092e-06  Weight :  [1.0015435]  B :  [-0.00350857]\n",
      "860 cost :  1.6118578e-06  Weight :  [1.0014709]  B :  [-0.00334372]\n",
      "880 cost :  1.4638084e-06  Weight :  [1.0014019]  B :  [-0.00318664]\n",
      "900 cost :  1.3293651e-06  Weight :  [1.0013361]  B :  [-0.00303695]\n",
      "920 cost :  1.2074481e-06  Weight :  [1.0012733]  B :  [-0.00289429]\n",
      "940 cost :  1.0966962e-06  Weight :  [1.0012134]  B :  [-0.00275834]\n",
      "960 cost :  9.96173e-07  Weight :  [1.0011566]  B :  [-0.00262882]\n",
      "980 cost :  9.048906e-07  Weight :  [1.0011023]  B :  [-0.0025054]\n",
      "1000 cost :  8.2193054e-07  Weight :  [1.0010506]  B :  [-0.00238778]\n",
      "1020 cost :  7.464421e-07  Weight :  [1.0010012]  B :  [-0.00227565]\n",
      "1040 cost :  6.780401e-07  Weight :  [1.0009542]  B :  [-0.00216878]\n",
      "1060 cost :  6.1585865e-07  Weight :  [1.0009093]  B :  [-0.00206692]\n",
      "1080 cost :  5.5938204e-07  Weight :  [1.0008665]  B :  [-0.0019699]\n",
      "1100 cost :  5.081122e-07  Weight :  [1.000826]  B :  [-0.00187747]\n",
      "1120 cost :  4.6164715e-07  Weight :  [1.0007874]  B :  [-0.00178936]\n",
      "1140 cost :  4.1925523e-07  Weight :  [1.0007504]  B :  [-0.00170536]\n",
      "1160 cost :  3.80829e-07  Weight :  [1.000715]  B :  [-0.00162531]\n",
      "1180 cost :  3.4594518e-07  Weight :  [1.0006816]  B :  [-0.00154912]\n",
      "1200 cost :  3.1430645e-07  Weight :  [1.0006498]  B :  [-0.00147641]\n",
      "1220 cost :  2.8548348e-07  Weight :  [1.0006189]  B :  [-0.00140718]\n",
      "1240 cost :  2.5934852e-07  Weight :  [1.0005903]  B :  [-0.00134123]\n",
      "1260 cost :  2.3559068e-07  Weight :  [1.0005624]  B :  [-0.00127827]\n",
      "1280 cost :  2.1403672e-07  Weight :  [1.0005362]  B :  [-0.00121841]\n",
      "1300 cost :  1.9441099e-07  Weight :  [1.0005109]  B :  [-0.0011612]\n",
      "1320 cost :  1.7658476e-07  Weight :  [1.0004871]  B :  [-0.00110682]\n",
      "1340 cost :  1.6038099e-07  Weight :  [1.0004642]  B :  [-0.00105483]\n",
      "1360 cost :  1.4577314e-07  Weight :  [1.0004426]  B :  [-0.00100547]\n",
      "1380 cost :  1.3236262e-07  Weight :  [1.0004215]  B :  [-0.00095829]\n",
      "1400 cost :  1.2031454e-07  Weight :  [1.0004022]  B :  [-0.00091341]\n",
      "1420 cost :  1.093062e-07  Weight :  [1.0003831]  B :  [-0.00087073]\n",
      "1440 cost :  9.9251565e-08  Weight :  [1.0003651]  B :  [-0.00082981]\n",
      "1460 cost :  9.0238636e-08  Weight :  [1.0003483]  B :  [-0.00079103]\n",
      "1480 cost :  8.198043e-08  Weight :  [1.0003316]  B :  [-0.00075402]\n",
      "1500 cost :  7.441382e-08  Weight :  [1.0003163]  B :  [-0.00071856]\n",
      "1520 cost :  6.768265e-08  Weight :  [1.0003017]  B :  [-0.000685]\n",
      "1540 cost :  6.1494156e-08  Weight :  [1.0002874]  B :  [-0.00065306]\n",
      "1560 cost :  5.5842396e-08  Weight :  [1.0002737]  B :  [-0.00062232]\n",
      "1580 cost :  5.07218e-08  Weight :  [1.0002612]  B :  [-0.00059315]\n",
      "1600 cost :  4.6101288e-08  Weight :  [1.0002491]  B :  [-0.00056553]\n",
      "1620 cost :  4.1886107e-08  Weight :  [1.0002372]  B :  [-0.00053915]\n",
      "1640 cost :  3.8023988e-08  Weight :  [1.0002259]  B :  [-0.00051372]\n",
      "1660 cost :  3.457252e-08  Weight :  [1.0002155]  B :  [-0.00048961]\n",
      "1680 cost :  3.1415208e-08  Weight :  [1.0002058]  B :  [-0.00046681]\n",
      "1700 cost :  2.8584253e-08  Weight :  [1.0001962]  B :  [-0.00044521]\n",
      "1720 cost :  2.59568e-08  Weight :  [1.0001867]  B :  [-0.00042444]\n",
      "1740 cost :  2.3562004e-08  Weight :  [1.0001777]  B :  [-0.00040438]\n",
      "1760 cost :  2.1412907e-08  Weight :  [1.0001696]  B :  [-0.00038536]\n",
      "1780 cost :  1.9462755e-08  Weight :  [1.0001619]  B :  [-0.00036738]\n",
      "1800 cost :  1.771996e-08  Weight :  [1.0001546]  B :  [-0.0003504]\n",
      "1820 cost :  1.6132645e-08  Weight :  [1.0001475]  B :  [-0.0003343]\n",
      "1840 cost :  1.46567976e-08  Weight :  [1.0001403]  B :  [-0.00031879]\n",
      "1860 cost :  1.3296305e-08  Weight :  [1.0001334]  B :  [-0.00030371]\n",
      "1880 cost :  1.2077791e-08  Weight :  [1.0001272]  B :  [-0.00028932]\n",
      "1900 cost :  1.096107e-08  Weight :  [1.0001214]  B :  [-0.00027571]\n",
      "1920 cost :  9.963155e-09  Weight :  [1.0001158]  B :  [-0.00026285]\n",
      "1940 cost :  9.068003e-09  Weight :  [1.0001106]  B :  [-0.00025069]\n",
      "1960 cost :  8.261575e-09  Weight :  [1.0001056]  B :  [-0.00023917]\n",
      "1980 cost :  7.5185715e-09  Weight :  [1.0001009]  B :  [-0.00022827]\n",
      "2000 cost :  6.8433423e-09  Weight :  [1.0000961]  B :  [-0.00021783]\n"
     ]
    }
   ],
   "source": [
    "x_train = tf.placeholder(tf.float32)\n",
    "y_train = tf.placeholder(tf.float32)\n",
    "\n",
    "\n",
    "#Variable : 텐서플로가 사용하는 변수이다.\n",
    "#텐서 플로우가 자체적으로 변경시키는 변수이다.\n",
    "\n",
    "# W : 값이 하나인 1차원 array. random 값이며 이름은 weight\n",
    "W = tf.Variable(tf.random_normal([1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "\n",
    "#node 생성\n",
    "hypothesis = x_train * W + b\n",
    "\n",
    "\n",
    "#cost 함수 : reduce_mean :  평균을 내주는 역할.\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - y_train))\n",
    "\n",
    "\n",
    "#cost를 최소화 하자.\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "\n",
    "#무엇을 최소화 할 것인가 -cost를 최소화\n",
    "# 텐서는 w와 b를 조정해서 최소화를 한다.\n",
    "# 그래프 상의 한 노드의 이름 : train\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "\n",
    "#실행하려면 세션을 만들어야 한다.\n",
    "sess = tf.Session()\n",
    "\n",
    "# variable을 사용하기 전에는 꼭 global_variables_initializer()로 변수를 초기화 해야한다.\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "# run을 실행시킬 때, 여러 개의 노드를 한 번에 실행시키기 위해서 리스트에 내용을 넣어서 처리한다.\n",
    "# 그 때 필요한 x와 ,y를 리스트로 넘겨서 줄 수 있다.\n",
    "# train으로 나오는 value는 필요가 없으니 _로 처리한다.\n",
    "\n",
    "for step in range(2001):\n",
    "    cost_val, W_val, b_val, _ = \\\n",
    "    sess.run([cost, W, b, train], \n",
    "                feed_dict={x_train: [1,2,3], y_train: [1,2,3]})\n",
    "    if step % 20 == 0:\n",
    "        print(step, \"cost : \", cost_val, \" Weight : \", W_val, \" B : \", b_val)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  cost :  9.668605  W :  [0.33758152]  b :  [0.90624756]\n",
      "20  cost :  0.0002118178  W :  [0.99946576]  b :  [1.0902517]\n",
      "40  cost :  1.2726482e-05  W :  [1.0022943]  b :  [1.091664]\n",
      "60  cost :  1.1110839e-05  W :  [1.0021567]  b :  [1.0922134]\n",
      "80  cost :  9.703191e-06  W :  [1.0020155]  b :  [1.0927233]\n",
      "100  cost :  8.47455e-06  W :  [1.0018835]  b :  [1.0931997]\n",
      "120  cost :  7.400293e-06  W :  [1.0017602]  b :  [1.0936451]\n",
      "140  cost :  6.462923e-06  W :  [1.001645]  b :  [1.0940613]\n",
      "160  cost :  5.64403e-06  W :  [1.0015372]  b :  [1.0944502]\n",
      "180  cost :  4.9287687e-06  W :  [1.0014365]  b :  [1.0948138]\n",
      "200  cost :  4.3040855e-06  W :  [1.0013424]  b :  [1.0951536]\n",
      "220  cost :  3.7587822e-06  W :  [1.0012546]  b :  [1.095471]\n",
      "240  cost :  3.282558e-06  W :  [1.0011723]  b :  [1.0957675]\n",
      "260  cost :  2.8671423e-06  W :  [1.0010957]  b :  [1.0960447]\n",
      "280  cost :  2.5033078e-06  W :  [1.0010239]  b :  [1.0963037]\n",
      "300  cost :  2.1864685e-06  W :  [1.0009569]  b :  [1.0965457]\n",
      "320  cost :  1.9095858e-06  W :  [1.0008942]  b :  [1.0967718]\n",
      "340  cost :  1.667701e-06  W :  [1.0008357]  b :  [1.0969832]\n",
      "360  cost :  1.4564819e-06  W :  [1.0007809]  b :  [1.0971807]\n",
      "380  cost :  1.2720335e-06  W :  [1.0007298]  b :  [1.0973653]\n",
      "400  cost :  1.1110608e-06  W :  [1.000682]  b :  [1.0975376]\n",
      "420  cost :  9.703211e-07  W :  [1.0006374]  b :  [1.0976988]\n",
      "440  cost :  8.475323e-07  W :  [1.0005957]  b :  [1.0978495]\n",
      "460  cost :  7.4008e-07  W :  [1.0005567]  b :  [1.0979903]\n",
      "480  cost :  6.4629285e-07  W :  [1.0005202]  b :  [1.0981219]\n",
      "500  cost :  5.6454917e-07  W :  [1.0004863]  b :  [1.0982448]\n",
      "520  cost :  4.928918e-07  W :  [1.0004544]  b :  [1.0983597]\n",
      "540  cost :  4.307385e-07  W :  [1.0004249]  b :  [1.098467]\n",
      "560  cost :  3.7622877e-07  W :  [1.000397]  b :  [1.0985671]\n",
      "580  cost :  3.2849502e-07  W :  [1.0003709]  b :  [1.0986608]\n",
      "600  cost :  2.870805e-07  W :  [1.0003467]  b :  [1.0987486]\n",
      "620  cost :  2.5065052e-07  W :  [1.000324]  b :  [1.0988303]\n",
      "640  cost :  2.1892879e-07  W :  [1.0003027]  b :  [1.0989069]\n",
      "660  cost :  1.9126578e-07  W :  [1.0002831]  b :  [1.0989783]\n",
      "680  cost :  1.6698829e-07  W :  [1.0002644]  b :  [1.0990452]\n",
      "700  cost :  1.4587395e-07  W :  [1.0002472]  b :  [1.0991075]\n",
      "720  cost :  1.2746591e-07  W :  [1.000231]  b :  [1.0991659]\n",
      "740  cost :  1.1135826e-07  W :  [1.000216]  b :  [1.0992204]\n",
      "760  cost :  9.7218404e-08  W :  [1.0002017]  b :  [1.0992714]\n",
      "780  cost :  8.4960085e-08  W :  [1.0001886]  b :  [1.0993191]\n",
      "800  cost :  7.4244916e-08  W :  [1.0001764]  b :  [1.0993634]\n",
      "820  cost :  6.479267e-08  W :  [1.0001647]  b :  [1.099405]\n",
      "840  cost :  5.660014e-08  W :  [1.000154]  b :  [1.099444]\n",
      "860  cost :  4.9469598e-08  W :  [1.0001441]  b :  [1.0994803]\n",
      "880  cost :  4.3225327e-08  W :  [1.0001345]  b :  [1.0995141]\n",
      "900  cost :  3.776672e-08  W :  [1.0001258]  b :  [1.099546]\n",
      "920  cost :  3.298569e-08  W :  [1.0001175]  b :  [1.0995756]\n",
      "940  cost :  2.8840214e-08  W :  [1.00011]  b :  [1.0996032]\n",
      "960  cost :  2.52051e-08  W :  [1.0001029]  b :  [1.099629]\n",
      "980  cost :  2.2051609e-08  W :  [1.000096]  b :  [1.0996529]\n",
      "1000  cost :  1.9251047e-08  W :  [1.0000898]  b :  [1.0996757]\n",
      "1020  cost :  1.680869e-08  W :  [1.0000839]  b :  [1.0996971]\n",
      "1040  cost :  1.4726413e-08  W :  [1.0000787]  b :  [1.0997164]\n",
      "1060  cost :  1.281037e-08  W :  [1.0000734]  b :  [1.0997353]\n",
      "1080  cost :  1.1245811e-08  W :  [1.0000687]  b :  [1.099752]\n",
      "1100  cost :  9.783571e-09  W :  [1.000064]  b :  [1.0997686]\n",
      "1120  cost :  8.5676675e-09  W :  [1.0000598]  b :  [1.0997835]\n",
      "1140  cost :  7.496612e-09  W :  [1.0000559]  b :  [1.0997978]\n",
      "1160  cost :  6.5231234e-09  W :  [1.0000522]  b :  [1.0998113]\n",
      "1180  cost :  5.7268608e-09  W :  [1.000049]  b :  [1.0998232]\n",
      "1200  cost :  4.967217e-09  W :  [1.0000458]  b :  [1.0998352]\n",
      "1220  cost :  4.3571275e-09  W :  [1.0000429]  b :  [1.0998455]\n",
      "1240  cost :  3.8445704e-09  W :  [1.0000403]  b :  [1.0998551]\n",
      "1260  cost :  3.3496803e-09  W :  [1.0000377]  b :  [1.0998646]\n",
      "1280  cost :  2.9246394e-09  W :  [1.0000352]  b :  [1.0998737]\n",
      "1300  cost :  2.5964937e-09  W :  [1.0000328]  b :  [1.0998808]\n",
      "1320  cost :  2.3013853e-09  W :  [1.0000309]  b :  [1.099888]\n",
      "1340  cost :  2.0163498e-09  W :  [1.0000288]  b :  [1.0998951]\n",
      "1360  cost :  1.7540515e-09  W :  [1.000027]  b :  [1.0999023]\n",
      "1380  cost :  1.493629e-09  W :  [1.000025]  b :  [1.0999094]\n",
      "1400  cost :  1.2964847e-09  W :  [1.0000232]  b :  [1.0999156]\n",
      "1420  cost :  1.1582415e-09  W :  [1.0000219]  b :  [1.0999204]\n",
      "1440  cost :  1.0293206e-09  W :  [1.0000206]  b :  [1.0999252]\n",
      "1460  cost :  9.016844e-10  W :  [1.0000193]  b :  [1.0999299]\n",
      "1480  cost :  7.7693585e-10  W :  [1.000018]  b :  [1.0999347]\n",
      "1500  cost :  6.707978e-10  W :  [1.0000167]  b :  [1.0999395]\n",
      "1520  cost :  5.6948013e-10  W :  [1.0000155]  b :  [1.0999442]\n",
      "1540  cost :  4.944923e-10  W :  [1.0000144]  b :  [1.0999479]\n",
      "1560  cost :  4.4815351e-10  W :  [1.0000137]  b :  [1.0999503]\n",
      "1580  cost :  4.1146678e-10  W :  [1.0000131]  b :  [1.0999527]\n",
      "1600  cost :  3.6561687e-10  W :  [1.0000124]  b :  [1.0999551]\n",
      "1620  cost :  3.2878233e-10  W :  [1.0000118]  b :  [1.0999575]\n",
      "1640  cost :  2.9749572e-10  W :  [1.0000111]  b :  [1.0999599]\n",
      "1660  cost :  2.6111593e-10  W :  [1.0000105]  b :  [1.0999622]\n",
      "1680  cost :  2.3084112e-10  W :  [1.0000099]  b :  [1.0999646]\n",
      "1700  cost :  1.9895197e-10  W :  [1.0000092]  b :  [1.099967]\n",
      "1720  cost :  1.7007551e-10  W :  [1.0000086]  b :  [1.0999694]\n",
      "1740  cost :  1.4789521e-10  W :  [1.0000079]  b :  [1.0999718]\n",
      "1760  cost :  1.2250893e-10  W :  [1.0000073]  b :  [1.0999742]\n",
      "1780  cost :  1.02056676e-10  W :  [1.0000066]  b :  [1.0999765]\n",
      "1800  cost :  8.385541e-11  W :  [1.000006]  b :  [1.0999788]\n",
      "1820  cost :  7.504468e-11  W :  [1.0000058]  b :  [1.0999795]\n",
      "1840  cost :  7.504468e-11  W :  [1.0000058]  b :  [1.0999795]\n",
      "1860  cost :  7.504468e-11  W :  [1.0000058]  b :  [1.0999795]\n",
      "1880  cost :  7.504468e-11  W :  [1.0000058]  b :  [1.0999795]\n",
      "1900  cost :  7.504468e-11  W :  [1.0000058]  b :  [1.0999795]\n",
      "1920  cost :  7.504468e-11  W :  [1.0000058]  b :  [1.0999795]\n",
      "1940  cost :  7.504468e-11  W :  [1.0000058]  b :  [1.0999795]\n",
      "1960  cost :  7.504468e-11  W :  [1.0000058]  b :  [1.0999795]\n",
      "1980  cost :  7.504468e-11  W :  [1.0000058]  b :  [1.0999795]\n",
      "2000  cost :  7.504468e-11  W :  [1.0000058]  b :  [1.0999795]\n",
      "[6.1000085]\n",
      "[3.5999942]\n",
      "[2.5999885 4.6      ]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "W = tf.Variable(tf.random_normal([1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "\n",
    "# shape는 1차원 배열이고 안의 내용물은 많이 넣을 수 있다.\n",
    "X = tf.placeholder(tf.float32, shape=[None])\n",
    "Y = tf.placeholder(tf.float32, shape=[None])\n",
    "\n",
    "# Our hypothesis XW+b ##이게 모델인 것 같습니다\n",
    "hypothesis = X * W + b\n",
    "\n",
    "# cost/Loss function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "\n",
    "# Minimize\n",
    "# gradientDescentOptimizer로 최소값을 구한다.\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "# Launch the graph in a session\n",
    "sess = tf.Session()\n",
    "\n",
    "#세선 초기화\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step in range(2001):\n",
    "    cost_val, W_val, b_val, _ = sess.run([cost, W, b, train],\n",
    "                                        feed_dict={\n",
    "                                            X: [1, 2, 3, 4, 5],\n",
    "                                            Y: [2.1, 3.1, 4.1, 5.1, 6.1]\n",
    "                                        })\n",
    "    if step % 20 == 0:\n",
    "        print(step, \" cost : \", cost_val, \" W : \", W_val, \" b : \", b_val)\n",
    "\n",
    "\n",
    "        \n",
    "# 잘 예측하는 지 확인해보기\n",
    "print(sess.run(hypothesis, feed_dict={X:[5]}))\n",
    "print(sess.run(hypothesis, feed_dict={X: [2.5]}))\n",
    "print(sess.run(hypothesis, feed_dict={X: [1.5, 3.5]}))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# cost의 minimize\n",
    "# 궁극적인 목표는 cost가 최저가 되는 w 를 찾자\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X= [1,2,3]\n",
    "Y = [1,2,3]\n",
    "\n",
    "# 왜 W를 받지? W를 조정해서 얻는 cost를 확인하고 싶어서. cost가 최소가 되는 w는 무엇일까\n",
    "W = tf.placeholder(tf.float32) #???????????\n",
    "\n",
    "# 가설\n",
    "hypothesis = X * W\n",
    "\n",
    "# cost function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "\n",
    "# session을 생성한다.\n",
    "sess = tf.Session()\n",
    "\n",
    "# session의 변수를 초기화\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "#배열\n",
    "W_val = []\n",
    "cost_val = []\n",
    "\n",
    "\n",
    "# 배열에다가 지금까지 학습한 모든 W와 b를 출력한다.\n",
    "# 그래프를 실행시키는 준비.\n",
    "\n",
    "for i in range(-30, 50):\n",
    "    feed_W = i * 0.1\n",
    "    curr_cost, curr_W = sess.run([cost,W], feed_dict={W: feed_W})\n",
    "    W_val.append(curr_W)\n",
    "    cost_val.append(curr_cost)\n",
    "\n",
    "\n",
    "    # plt 뭐하는 걸까용 W_val이 x축, cost_val 이  y축 -> 그래야 어떤 모양을 하고 있는지를 볼 수 있기 때문이다.\n",
    "plt.plot(W_val, cost_val)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#이 최소의 값을 자동으로 찾을 수는 없을까?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.2317706 [0.8713336]\n",
      "1 0.06592592 [0.9313779]\n",
      "2 0.018752245 [0.96340156]\n",
      "3 0.0053339684 [0.98048085]\n",
      "4 0.0015172013 [0.9895898]\n",
      "5 0.0004315654 [0.9944479]\n",
      "6 0.00012275245 [0.9970389]\n",
      "7 3.491796e-05 [0.9984207]\n",
      "8 9.931638e-06 [0.9991577]\n",
      "9 2.8255954e-06 [0.99955076]\n",
      "10 8.0378584e-07 [0.9997604]\n",
      "11 2.2863242e-07 [0.9998722]\n",
      "12 6.4955785e-08 [0.9999319]\n",
      "13 1.8533527e-08 [0.99996364]\n",
      "14 5.2605067e-09 [0.9999806]\n",
      "15 1.4922996e-09 [0.9999897]\n",
      "16 4.2098236e-10 [0.9999945]\n",
      "17 1.2046897e-10 [0.9999971]\n",
      "18 3.474554e-11 [0.99999845]\n",
      "19 9.166001e-12 [0.99999917]\n",
      "20 3.1832315e-12 [0.9999995]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "x_data = [1,2,3]\n",
    "y_data = [1,2,3]\n",
    "\n",
    "W = tf.Variable(tf.random_normal([1]), name='weight')\n",
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "\n",
    "#가설을 세우자\n",
    "hypothesis = W * X\n",
    "\n",
    "# cost/loss 함수\n",
    "#@@ reduce sum과 reduce mean의 차이는 무엇인가??\n",
    "\n",
    "cost = tf.reduce_sum(tf.square(hypothesis - Y))\n",
    "\n",
    "\n",
    "# 최소화\n",
    "# tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "# gradientDescentOptimizer로 최소값을 구한다.\n",
    "#   optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "#   train = optimizer.minimize(cost)\n",
    "\n",
    "# gradient descent를 수행하는데, 이제까지는 메소드 하나로 잘 했지만, 이제는 직접 구현해보자.\n",
    "learning_rate = 0.1\n",
    "gradient = tf.reduce_mean((W * X - Y ) * X)\n",
    "descent = W - learning_rate * gradient\n",
    "update = W.assign(descent)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step in range(21):\n",
    "    sess.run(update, feed_dict={X:x_data, Y:y_data})\n",
    "    print(step, sess.run(cost, feed_dict={X:x_data, Y:y_data}), sess.run(W))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 5.0\n",
      "1 3.264\n",
      "2 3.1482668\n",
      "3 3.140551\n",
      "4 3.1400366\n",
      "5 3.1400025\n",
      "6 3.14\n",
      "7 3.14\n",
      "8 3.14\n",
      "9 3.14\n",
      "10 3.14\n",
      "11 3.14\n",
      "12 3.14\n",
      "13 3.14\n",
      "14 3.14\n",
      "15 3.14\n",
      "16 3.14\n",
      "17 3.14\n",
      "18 3.14\n",
      "19 3.14\n",
      "20 3.14\n",
      "21 3.14\n",
      "22 3.14\n",
      "23 3.14\n",
      "24 3.14\n",
      "25 3.14\n",
      "26 3.14\n",
      "27 3.14\n",
      "28 3.14\n",
      "29 3.14\n",
      "30 3.14\n",
      "31 3.14\n",
      "32 3.14\n",
      "33 3.14\n",
      "34 3.14\n",
      "35 3.14\n",
      "36 3.14\n",
      "37 3.14\n",
      "38 3.14\n",
      "39 3.14\n",
      "40 3.14\n",
      "41 3.14\n",
      "42 3.14\n",
      "43 3.14\n",
      "44 3.14\n",
      "45 3.14\n",
      "46 3.14\n",
      "47 3.14\n",
      "48 3.14\n",
      "49 3.14\n",
      "50 3.14\n",
      "51 3.14\n",
      "52 3.14\n",
      "53 3.14\n",
      "54 3.14\n",
      "55 3.14\n",
      "56 3.14\n",
      "57 3.14\n",
      "58 3.14\n",
      "59 3.14\n",
      "60 3.14\n",
      "61 3.14\n",
      "62 3.14\n",
      "63 3.14\n",
      "64 3.14\n",
      "65 3.14\n",
      "66 3.14\n",
      "67 3.14\n",
      "68 3.14\n",
      "69 3.14\n",
      "70 3.14\n",
      "71 3.14\n",
      "72 3.14\n",
      "73 3.14\n",
      "74 3.14\n",
      "75 3.14\n",
      "76 3.14\n",
      "77 3.14\n",
      "78 3.14\n",
      "79 3.14\n",
      "80 3.14\n",
      "81 3.14\n",
      "82 3.14\n",
      "83 3.14\n",
      "84 3.14\n",
      "85 3.14\n",
      "86 3.14\n",
      "87 3.14\n",
      "88 3.14\n",
      "89 3.14\n",
      "90 3.14\n",
      "91 3.14\n",
      "92 3.14\n",
      "93 3.14\n",
      "94 3.14\n",
      "95 3.14\n",
      "96 3.14\n",
      "97 3.14\n",
      "98 3.14\n",
      "99 3.14\n"
     ]
    }
   ],
   "source": [
    "#머신러닝, 정말 동작되긴 하는 거야?\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "#예상되는 W의 값 : 3.14\n",
    "X = [1, 2, 3]\n",
    "Y = [3.14, 6.28, 9.42]\n",
    "\n",
    "# Set wrong model weights\n",
    "#나 분명히 W를 5.0으로 주었다!\n",
    "W = tf.Variable(5.0)\n",
    "\n",
    "#Linear nodel\n",
    "# 가설 식은 이렇고.\n",
    "hypothesis = X * W\n",
    "\n",
    "# @@ 이거 차이를 구하는 거랑, gradientdescent로 최솟값을 구하는 거랑 의미가 좀 다른가?\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "\n",
    "# cost/loss function\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "# Launch the graph in a session\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step in range(100):\n",
    "    print(step, sess.run(W))\n",
    "    sess.run(train)\n",
    "\n",
    "# sess.run(W) 하면 W가 한번 더 돌아가는 거 아님? -> 이렇게 출력해도 되나?>>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
